# FaultMaven Self-Hosted Configuration
# Copy this file to .env and add your LLM API key
#
# IMPORTANT: This configuration prioritizes simplicity for self-hosted deployments.
# For advanced features like task-specific routing and cost optimization,
# see fm-agent-service/.env.example in the development repository.

# ============================================================================
# LLM Provider Configuration (at least one required)
# ============================================================================
# FaultMaven supports 7 LLM providers with automatic fallback.
# The simplest setup: configure ONE provider and it works for all tasks.
#
# Supported Providers:
# - Cloud LLMs: OpenAI, Anthropic, Groq, Gemini, Fireworks, OpenRouter
# - Local LLMs: Ollama, LM Studio, LocalAI, vLLM (free, private, runs on your machine)
#
# Get API keys from:
# - OpenAI: https://platform.openai.com/api-keys
# - Anthropic: https://console.anthropic.com/
# - Groq: https://console.groq.com/ (FREE tier available - ultra-fast!)
# - Gemini: https://makersuite.google.com/app/apikey
# - Fireworks: https://fireworks.ai/api-keys
# - OpenRouter: https://openrouter.ai/keys (aggregates multiple providers)

# ============================================================================
# BASIC CONFIGURATION - One LLM for All Tasks (Recommended)
# ============================================================================
# Choose ONE option below. This LLM will handle all AI tasks in FaultMaven.
# The service automatically uses this provider for chat, knowledge base queries,
# and all other AI operations. No additional configuration needed!
# ============================================================================

# Option 1: Cloud LLM (best performance, pay-per-use)
OPENAI_API_KEY=sk-...                    # Uncomment and add your key
# ANTHROPIC_API_KEY=sk-ant-...
# GROQ_API_KEY=gsk_...                   # FREE tier available!
# GEMINI_API_KEY=AIza...
# FIREWORKS_API_KEY=fw_...
# OPENROUTER_API_KEY=sk-or-...

# Option 2: Local LLM (FREE, private, runs on your machine)
# LOCAL_LLM_API_KEY=not-needed           # Use "not-needed" for no-auth servers
# LOCAL_LLM_URL=http://localhost:11434/v1
# LOCAL_LLM_MODEL=llama3.1
#
# Quick Setup (Ollama - easiest):
#   1. Install: https://ollama.ai/download
#   2. Run: ollama serve
#   3. Pull model: ollama pull llama3.1
#   4. Uncomment 3 LOCAL_LLM variables above
#   5. Comment out cloud provider keys
#
# Other local servers (use same LOCAL_LLM variables):
#   - LM Studio: http://localhost:1234/v1
#   - LocalAI: http://localhost:8080/v1
#   - vLLM: http://localhost:8000/v1

# ============================================================================
# OPTIONAL: Multiple Providers (Fallback and Redundancy)
# ============================================================================
# Add multiple API keys for automatic fallback if one provider fails.
# FaultMaven tries providers in order until one succeeds.
# Recommended for production reliability.
#
# Example - Cloud + Free Cloud fallback:
#   OPENAI_API_KEY=sk-...              # Primary
#   GROQ_API_KEY=gsk_...               # Fallback (FREE tier)
#
# Example - Cloud + Local fallback:
#   OPENAI_API_KEY=sk-...              # Primary
#   LOCAL_LLM_API_KEY=not-needed       # Always-available fallback
#   LOCAL_LLM_URL=http://localhost:11434/v1
#   LOCAL_LLM_MODEL=llama3.1

# ============================================================================
# ADVANCED: Model Override (Optional)
# ============================================================================
# Override the default model for any provider.
# Useful for testing newer models or cost optimization.
#
# OPENAI_MODEL=gpt-4o-mini
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
# GROQ_MODEL=llama-3.3-70b-versatile
# GEMINI_MODEL=gemini-1.5-pro
# FIREWORKS_MODEL=accounts/fireworks/models/llama-v3p1-70b-instruct
# OPENROUTER_MODEL=anthropic/claude-3.5-sonnet
# LOCAL_LLM_MODEL=llama3.1

# ============================================================================
# ADVANCED: Base URL Override (Optional)
# ============================================================================
# Only needed for Azure OpenAI, custom proxies, or alternative endpoints.
#
# OPENAI_BASE_URL=https://api.openai.com/v1
# ANTHROPIC_BASE_URL=https://api.anthropic.com/v1
# GROQ_BASE_URL=https://api.groq.com/openai/v1
# GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta
# FIREWORKS_BASE_URL=https://api.fireworks.ai/inference/v1
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
# LOCAL_LLM_URL=http://localhost:11434/v1

# ============================================================================
# Deployment Profile (DO NOT CHANGE for self-hosted)
# ============================================================================
PROFILE=public

# ============================================================================
# Database Configuration (SQLite - zero config)
# ============================================================================
DB_TYPE=sqlite
DATABASE_URL=sqlite+aiosqlite:////data/faultmaven.db

# ============================================================================
# Infrastructure Services (configured by docker-compose)
# ============================================================================
REDIS_HOST=redis
REDIS_PORT=6379

CHROMADB_HOST=chromadb
CHROMADB_PORT=8000

# ============================================================================
# Service URLs (internal Docker network)
# ============================================================================
# These are used by individual services for inter-service communication
KNOWLEDGE_SERVICE_URL=http://fm-knowledge-service:8000
CASE_SERVICE_URL=http://fm-case-service:8000
EVIDENCE_SERVICE_URL=http://fm-evidence-service:8000
AUTH_SERVICE_URL=http://fm-auth-service:8000
SESSION_SERVICE_URL=http://fm-session-service:8000

# ============================================================================
# API Gateway Configuration
# ============================================================================
# API Gateway is the single entry point for all client requests
# Browser extension and dashboard connect to this port
# External URL: http://SERVER_HOST:8090
API_GATEWAY_PORT=8090

# API Gateway service URLs (used for routing and OpenAPI aggregation)
# These environment variables are prefixed with FM_ to match gateway config
FM_AUTH_SERVICE_URL=http://fm-auth-service:8000
FM_SESSION_SERVICE_URL=http://fm-session-service:8000
FM_CASE_SERVICE_URL=http://fm-case-service:8000
FM_KNOWLEDGE_SERVICE_URL=http://fm-knowledge-service:8000
FM_EVIDENCE_SERVICE_URL=http://fm-evidence-service:8000
FM_AGENT_SERVICE_URL=http://fm-agent-service:8000
# FM_INVESTIGATION_SERVICE_URL=http://fm-investigation-service:8000  # Not yet deployed

# ============================================================================
# Storage Configuration
# ============================================================================
STORAGE_TYPE=local
LOCAL_UPLOAD_DIR=/data/uploads

# ============================================================================
# Network Access Configuration
# ============================================================================
# Set this to your server's IP address or hostname
# This value is baked into the dashboard's JavaScript at build time
#
# Examples:
#   - Local network: SERVER_HOST=192.168.0.200
#   - Hostname: SERVER_HOST=faultmaven.local
#   - Public IP: SERVER_HOST=203.0.113.42
#
# IMPORTANT: Cannot use 'localhost' here!
# Why? When you open the dashboard in a browser on a DIFFERENT machine,
# 'localhost' would refer to that machine, not your FaultMaven server.
#
# Only use 'localhost' if you will ONLY access the dashboard from the
# same machine where FaultMaven is running (rare for headless servers).

SERVER_HOST=192.168.0.200

# ============================================================================
# Authentication (Self-Hosted Simplified Auth)
# ============================================================================
# Simple username/password authentication for dashboard and browser extension
# IMPORTANT: Change these credentials before deploying to production!

DASHBOARD_USERNAME=admin
DASHBOARD_PASSWORD=changeme123

# Optional: Set a default user token for headless mode (browser extension)
# If not set, users must login through dashboard first
# DEFAULT_USER_TOKEN=my-secret-token-here
