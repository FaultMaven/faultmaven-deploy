# FaultMaven Self-Hosted Configuration
# Copy this file to .env and add your LLM API key

# ============================================================================
# LLM Provider Configuration (at least one required)
# ============================================================================
# FaultMaven supports 7 LLM providers with automatic fallback.
# The simplest setup: configure ONE provider and it works for all tasks.
#
# Supported Providers:
# - Cloud LLMs: OpenAI, Anthropic, Groq, Gemini, Fireworks, OpenRouter
# - Local LLMs: Ollama, LM Studio, LocalAI, vLLM (free, private, runs on your machine)
#
# Get API keys from:
# - OpenAI: https://platform.openai.com/api-keys
# - Anthropic: https://console.anthropic.com/
# - Groq: https://console.groq.com/ (FREE tier available - ultra-fast!)
# - Gemini: https://makersuite.google.com/app/apikey
# - Fireworks: https://fireworks.ai/api-keys
# - OpenRouter: https://openrouter.ai/keys (aggregates multiple providers)

# ============================================================================
# BASIC CONFIGURATION - One LLM for All Tasks (Recommended)
# ============================================================================
# Choose ONE option below. This LLM will handle all AI tasks in FaultMaven.

# Option 1: Cloud LLM (OpenAI, Anthropic, etc.)
OPENAI_API_KEY=sk-...
# ANTHROPIC_API_KEY=sk-ant-...
# GROQ_API_KEY=gsk_...
# GEMINI_API_KEY=AIza...
# FIREWORKS_API_KEY=fw_...
# OPENROUTER_API_KEY=sk-or-...

# Option 2: Local LLM (FREE, private, runs locally)
# LOCAL_LLM_API_KEY=not-needed
# LOCAL_LLM_URL=http://localhost:11434/v1
# LOCAL_LLM_MODEL=llama3.1
#
# Setup Instructions:
# 1. Install Ollama: https://ollama.ai/download
# 2. Run: ollama serve
# 3. Pull a model: ollama pull llama3.1
# 4. Uncomment the 3 LOCAL_LLM variables above
# 5. Comment out any cloud provider API keys
#
# Other local LLM servers (use same variables):
# - LM Studio: http://localhost:1234/v1
# - LocalAI: http://localhost:8080/v1
# - vLLM: http://localhost:8000/v1

# ============================================================================
# OPTIONAL: Multiple Providers (Fallback and Redundancy)
# ============================================================================
# Configure multiple providers for automatic fallback if one fails.
# Just add additional API keys - FaultMaven will try them in order.
#
# Example: Primary OpenAI, fallback to Groq
# OPENAI_API_KEY=sk-...
# GROQ_API_KEY=gsk_...

# ============================================================================
# ADVANCED: Model Override (Optional)
# ============================================================================
# Override the default model for any provider.
# Useful for testing newer models or cost optimization.
#
# OPENAI_MODEL=gpt-4o-mini
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
# GROQ_MODEL=llama-3.3-70b-versatile
# GEMINI_MODEL=gemini-1.5-pro
# FIREWORKS_MODEL=accounts/fireworks/models/llama-v3p1-70b-instruct
# OPENROUTER_MODEL=anthropic/claude-3.5-sonnet
# LOCAL_LLM_MODEL=llama3.1

# ============================================================================
# ADVANCED: Base URL Override (Optional)
# ============================================================================
# Only needed for Azure OpenAI, custom proxies, or alternative endpoints.
#
# OPENAI_BASE_URL=https://api.openai.com/v1
# ANTHROPIC_BASE_URL=https://api.anthropic.com/v1
# GROQ_BASE_URL=https://api.groq.com/openai/v1
# GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta
# FIREWORKS_BASE_URL=https://api.fireworks.ai/inference/v1
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
# LOCAL_LLM_URL=http://localhost:11434/v1

# ============================================================================
# ADVANCED: Task-Specific Provider Routing (Optional)
# ============================================================================
# For cost optimization, assign specific providers to different task types.
# By default, FaultMaven uses the same provider for all tasks (simplest setup).
#
# Task Types:
#   - CHAT: Main diagnostic conversations (most frequent)
#   - MULTIMODAL: Visual evidence processing (images, screenshots) [future feature]
#   - SYNTHESIS: Knowledge base RAG queries (high volume) [future feature]
#
# Example 1: Same provider, different models for cost optimization
# CHAT_PROVIDER=openai
# CHAT_MODEL=gpt-4o                    # Premium model for chat
#
# SYNTHESIS_PROVIDER=openai
# SYNTHESIS_MODEL=gpt-4o-mini          # Cheaper model for RAG (10x savings!)
#
# Example 2: Mix cloud and local LLMs
# CHAT_PROVIDER=openai
# CHAT_MODEL=gpt-4o                    # Cloud LLM for complex chat
#
# SYNTHESIS_PROVIDER=local
# SYNTHESIS_MODEL=llama3.1             # Local LLM for RAG (free!)
#
# Example 3: Different providers for different tasks
# CHAT_PROVIDER=anthropic
# CHAT_MODEL=claude-3-5-sonnet-20241022
#
# MULTIMODAL_PROVIDER=gemini
# MULTIMODAL_MODEL=gemini-1.5-pro      # Excellent vision capabilities
#
# SYNTHESIS_PROVIDER=groq
# SYNTHESIS_MODEL=llama-3.1-8b-instant # FREE tier, ultra-fast
#
# STRICT_PROVIDER_MODE=false           # If true, fails instead of falling back

# ============================================================================
# Deployment Profile (DO NOT CHANGE for self-hosted)
# ============================================================================
PROFILE=public

# ============================================================================
# Database Configuration (SQLite - zero config)
# ============================================================================
DB_TYPE=sqlite
DATABASE_URL=sqlite+aiosqlite:////data/faultmaven.db

# ============================================================================
# Infrastructure Services (configured by docker-compose)
# ============================================================================
REDIS_HOST=redis
REDIS_PORT=6379

CHROMADB_HOST=chromadb
CHROMADB_PORT=8000

# ============================================================================
# Service URLs (internal Docker network)
# ============================================================================
KNOWLEDGE_SERVICE_URL=http://fm-knowledge-service:8000
CASE_SERVICE_URL=http://fm-case-service:8000
EVIDENCE_SERVICE_URL=http://fm-evidence-service:8000
AUTH_SERVICE_URL=http://fm-auth-service:8000
SESSION_SERVICE_URL=http://fm-session-service:8000

# ============================================================================
# Storage Configuration
# ============================================================================
STORAGE_TYPE=local
LOCAL_UPLOAD_DIR=/data/uploads

# ============================================================================
# Network Access Configuration
# ============================================================================
# Set this to your server's IP address or hostname
# This value is baked into the dashboard's JavaScript at build time
#
# Examples:
#   - Local network: SERVER_HOST=192.168.0.200
#   - Hostname: SERVER_HOST=faultmaven.local
#   - Public IP: SERVER_HOST=203.0.113.42
#
# IMPORTANT: Cannot use 'localhost' here!
# Why? When you open the dashboard in a browser on a DIFFERENT machine,
# 'localhost' would refer to that machine, not your FaultMaven server.
#
# Only use 'localhost' if you will ONLY access the dashboard from the
# same machine where FaultMaven is running (rare for headless servers).

SERVER_HOST=192.168.0.200

# ============================================================================
# Authentication (Self-Hosted Simplified Auth)
# ============================================================================
# Simple username/password authentication for dashboard and browser extension
# IMPORTANT: Change these credentials before deploying to production!

DASHBOARD_USERNAME=admin
DASHBOARD_PASSWORD=changeme123

# Optional: Set a default user token for headless mode (browser extension)
# If not set, users must login through dashboard first
# DEFAULT_USER_TOKEN=my-secret-token-here
